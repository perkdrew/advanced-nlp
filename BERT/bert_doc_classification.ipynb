{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_doc_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeG6OKTJh5Em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62366140-5bb4-421e-d78c-1633fc111715"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAtsZNZFiV6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afe6038a-e17e-4872-87fb-a2a12336f024"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  # tell Pytorch to use the GPU\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "  print('We will use the GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHeNlM3ijGGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "a3f37498-bd1a-4f86-c545-cdbf8f6750c5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRHA080xjMF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "import os\n",
        "\n",
        "if not os.path.exists('./data/'):\n",
        "  os.mkdir('./data/')\n",
        "\n",
        "files = [\n",
        "         ('./data/attack_annotated_comments.tsv', 'https://ndownloader.figshare.com/files/7554634'),\n",
        "         ('./data/attack_annotations.tsv',        'https://ndownloader.figshare.com/files/7554637')\n",
        "]\n",
        "\n",
        "for (filename, url) in files:\n",
        "  if not os.path.exists(filename):\n",
        "    print('Downloading:', filename)\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    print('Done!')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wt2TtAYkmKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ceee3e04-6197-4e01-d415-d7660af9d5d1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('Parsing the dataset .tsv file...')\n",
        "comments = pd.read_csv('./data/attack_annotated_comments.tsv', sep='\\t', index_col=0)\n",
        "annotations = pd.read_csv('./data/attack_annotations.tsv', sep='\\t')\n",
        "print('Done!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing the dataset .tsv file...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kWCIzCnlB_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "945f3520-5e13-47ce-9482-734ee9775a7c"
      },
      "source": [
        "comments.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>year</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>ns</th>\n",
              "      <th>sample</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37675</th>\n",
              "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
              "      <td>2002</td>\n",
              "      <td>False</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44816</th>\n",
              "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
              "      <td>2002</td>\n",
              "      <td>False</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49851</th>\n",
              "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
              "      <td>2002</td>\n",
              "      <td>False</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89320</th>\n",
              "      <td>Next, maybe you could work on being less cond...</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93890</th>\n",
              "      <td>This page will need disambiguation.</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  comment  year  ...  sample  split\n",
              "rev_id                                                           ...               \n",
              "37675   `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002  ...  random  train\n",
              "44816   `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002  ...  random  train\n",
              "49851   NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002  ...  random  train\n",
              "89320    Next, maybe you could work on being less cond...  2002  ...  random    dev\n",
              "93890                This page will need disambiguation.   2002  ...  random  train\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrpHB65RlIvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "544b9787-1b21-4d10-a09f-cd711dd3f7c8"
      },
      "source": [
        "comments[['comment', 'split']].groupby('split').count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>split</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>dev</th>\n",
              "      <td>23160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>23178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>69526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       comment\n",
              "split         \n",
              "dev      23160\n",
              "test     23178\n",
              "train    69526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9IXj22klP_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create labels and join them with comments\n",
        "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
        "\n",
        "comments['attack'] = labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkX8he6Xnov3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace('NEWLINE_TOKEN', ' '))\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace('TAB_TOKEN', ' '))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHzbkRrun9F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_comments = comments.query(\"split=='train'\")\n",
        "test_comments = comments.query(\"split=='test'\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPwJJnUXoS6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c84d11c6-99bd-478c-bfc8-c060198ca71c"
      },
      "source": [
        "import textwrap\n",
        "import random\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=80)\n",
        "\n",
        "attack_examples = train_comments.query('attack')['comment']\n",
        "\n",
        "for i in range(10):\n",
        "  j = random.choice(attack_examples.index)\n",
        "\n",
        "  print('')\n",
        "  print(wrapper.fill(attack_examples[j]))\n",
        "  print('')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Get a life brotha== fick me looking at sum of the shit u been doin. are you\n",
            "queer cunt or what?  and i quote  As I'm currently away from my home ADSL\n",
            "network, I'll be limited since I'm now using a bandwith limited mobile limited\n",
            "connection. I hope to be back when the issue is fixed.  if that dont scream a\n",
            "fucking wanker i don't know what does.  ==\n",
            "\n",
            "\n",
            "  == You are == Spastic guy, that don't want to roll over and do what I\n",
            "suggested moment ago....\n",
            "\n",
            "\n",
            "  == CENSORSHIP ==  Hi Toure Neblett, I realize you think you are above\n",
            "criticism on account of your complexion and your hatred of your white relatives.\n",
            "But suck it in and quit censoring little boy.  If you can't reply to debate,\n",
            "throwing a tantrum is immature.  Oh, and Janet Jackson will never ever sleep\n",
            "with you!\n",
            "\n",
            "\n",
            "  == Fuck You ==  Nigga, fuck you.  Flocka aint a gangster, so he's not Gangsta\n",
            "rap!  Kids these days are so fucking stupid and mainstream..\n",
            "\n",
            "\n",
            " :This is either a completely failed attempt at joking sarcasm, or you're a\n",
            "hypocrite.\n",
            "\n",
            "\n",
            "  block me, this is a public library, il be gone in 10 minutes so i dont give a\n",
            "shit\n",
            "\n",
            "\n",
            "`  == Are you retarded or what?? ==  So what you want?   You think PSP Wiki is\n",
            "have trusted sources? Its copy of Wikipedia and users are adding information not\n",
            "Sony.   In PSP Wiki is written: The latest reported firmware version shipping\n",
            "pre-installed on new PSPs is 5.50 (confirmed by a few people who buyed a\n",
            "PSP-3000 Special Edition (Harry Potter Version).    This have trusted\n",
            "sources???: (confirmed by a few people who buyed a PSP-3000 Special Edition\n",
            "(Harry Potter Version)?? OH YES SONY CONFIRMED THAT!!! OR WHAT?? [and even world\n",
            "bought is written wrong]   Actually a new PSPs have a version 5.70 or higher.\n",
            "Even in Wikipedia is written: [5.70]This firmware is only available pre-\n",
            "installed on new PSP-N1000 (PSPgo) models and cannot be downloaded. SO ARE YOU\n",
            "BLIND OR WHAT???!!! AND HOW CAN BE THIS POSTED IF IT DOESN'T HAVE SOURCE?   And\n",
            "5.70 is also installed on new PSPs 3000. I can confirm that. On PSPs came with\n",
            "limited edition GT bundle. I bought one.(already sold it because its not\n",
            "hackable)   If you still don't belive me read there:\n",
            "http://forums.pspslimhacks.com/user-help/3594-psp-3000-firmware-5-70-a.html\n",
            "http://forums.mformature.net/showthread.php?t=6088      This is not fear: few\n",
            "millions people read pages in Wikipedia but actually one user who thinks he is\n",
            "god is editing as his ass wants.      SO IF CITATION ````[5.70]This firmware is\n",
            "only available pre-installed on new PSP-N1000 (PSPgo) models and cannot be\n",
            "downloaded.[THIS CAN BE MY SOURCE FOR PSPs GO]```` AND INFORMATION IN LINKS\n",
            "DOESN'T HAVE SOURCE THERE IS ONE POSSIBILITY YOU DON'T KNOW WHAT IS SOURCE AND\n",
            "YOU ARE TOTALLY STUPIDLY RETARDED.      SORRY BUT TRUE.`\n",
            "\n",
            "\n",
            "  Hey fuck you Hal Turner sent a death threat to me in teh mail yesterday. He's\n",
            "a RACIST Don't let him use this is a launch pad for thi NAZI ANTICS!\n",
            "\n",
            "\n",
            "`  Do you do this ``administrator`` stuff for free.  Who works for free?!?!\n",
            "Only dumb cunts!!!!`\n",
            "\n",
            "\n",
            "eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock\n",
            "you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my\n",
            "cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat\n",
            "my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock\n",
            "you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my\n",
            "cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat\n",
            "my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock\n",
            "you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my\n",
            "cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat\n",
            "my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock\n",
            "you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my\n",
            "cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat\n",
            "my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock\n",
            "you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my\n",
            "cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat\n",
            "my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock\n",
            "you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my\n",
            "cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!eat\n",
            "my cock you dirty whore!eat my cock you dirty whore!eat my cock you dirty\n",
            "whore!eat my cock you dirty whore!eat my cock you dirty whore!eat my cock you\n",
            "dirty whore!eat my cock you dirty whore!eat my cock you dirty whore!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHVavFsBqWT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36428ac0-7103-48d1-91f6-268d31e6aea4"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBu-v8elqlLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "6c81303d-09d8-4709-b922-bf1770f5a7dd"
      },
      "source": [
        "text = train_comments.iloc[0].comment\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# print out the list of tokens and see what we truncate\n",
        "print('==== First 512 tokens: ====\\n')\n",
        "print(wrapper.fill(str(' '.join(tokens[0:512]))))\n",
        "print('')\n",
        "print('\\n==== Remaining {:,} tokens: ====\\n'.format(len(tokens) - 512))\n",
        "print(wrapper.fill(str(' '.join(tokens[512:]))))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== First 512 tokens: ====\n",
            "\n",
            "` - this is not ` ` creative ` ` . those are the dictionary definitions of the\n",
            "terms ` ` insurance ` ` and ` ` en ##sur ##ance ` ` as properly applied to ` `\n",
            "destruction ` ` . if you don ' t understand that , fine , legitimate criticism ,\n",
            "i ' ll write up ` ` three man cell ` ` and ` ` bounty hunter ` ` and then it\n",
            "will be easy to understand why ` ` ensured ` ` and ` ` ins ##ured ` ` are\n",
            "different - and why both differ from ` ` assured ` ` . the sentence you quote is\n",
            "absolutely neutral . you just aren ' t familiar with the underlying theory of\n",
            "strike - back ( e . g . submarines as employed in nuclear warfare ) guiding the\n",
            "insurance , nor likely the three man cell structure that kept the ira from being\n",
            "broken by the british . if that ' s my fault , fine , i can fix that to explain\n",
            ". but the ##r ' es nothing ` ` personal ` ` or ` ` creative ` ` about it . i ' m\n",
            "tired of arguing with you . re : the other article , ` ` multi - party ` ` turns\n",
            "up plenty , and there is more use of ` ` mutually ` ` than ` ` mutual ` ` . if i\n",
            "were to apply your standard i ' d be moving ` ` mutual assured destruction ` `\n",
            "to ` ` talk ` ` for not appealing to a reagan voter ' s bias ##es about its\n",
            "effectiveness , and for dropping the ` ` l ##y ` ` . there is a double standard\n",
            "in your edit ##s . if it comes from some us history book , like ` ` peace\n",
            "movement ` ` or ' m . a . d . ' as defined in 1950 , you like it , even if the\n",
            "definition is totally useless in 2002 and only of historical interest . if it\n",
            "makes any even - obvious connection or implication from the language chosen in\n",
            "multiple profession - specific terms , you consider it somehow non - neutral . .\n",
            ". gandhi thinks ` ` eye for an eye ` ` describes riots , death penalty , and war\n",
            "all at once , but you don ' t . what do you know that gandhi doesn ' t ? guess\n",
            "what : reality is not neutral . current use of terms is slightly more\n",
            "controversial . neutrality requires negotiation , and some willingness to learn\n",
            ". this is your problem not mine . you may dislike the writing , fine , that can\n",
            "be fixed . but disregard ##ing fundamental ax ##ioms of phil ##os ##phy with\n",
            "names that rec ##ur in multiple phrases , or failing to make critical\n",
            "distinctions like ' insurance ' versus ' assurance ' versus ' en ##sur ##ance '\n",
            "( which\n",
            "\n",
            "\n",
            "==== Remaining 79 tokens: ====\n",
            "\n",
            "are made in one quote by an air force general in an in - context quote ) , is\n",
            "just a di ##sser ##vic ##e to the reader . if someone comes here to research a\n",
            "topic like mad , they want some context , beyond history . if this is a history\n",
            "book , fine , it ' s a history book . but that wasn ' t what it was claimed to\n",
            "be . . . `\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlslQyjdr8z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A few ways to deal with this problem is truncation and chunking\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "input_ids = []\n",
        "lengths = []\n",
        "\n",
        "print('Tokenizing comments...')\n",
        "for sent in train_comments.comment:\n",
        "  if ((len(input_ids) % 20000) == 0):\n",
        "    print('Read {:,} comments.'.format(len(input_ids)))\n",
        "    \n",
        "  # 'encode' will:\n",
        "  # (1) tokenize the sentence\n",
        "  # (2) prepend the [CLS] token to the start\n",
        "  # (3) append the [SEP] token to the start\n",
        "  # (4) map tokens to their IDs\n",
        "  encoded_sent = tokenizer.encode(\n",
        "        sent,                      # sentence to encode\n",
        "        add_special_tokens = True, # add [CLS] and [SEP]\n",
        "        #max_length = 512,\n",
        "        #return_tensors = 'pt',\n",
        "    )\n",
        "\n",
        "  input_ids.append(encoded_sent)\n",
        "  lengths.append(len(encoded_sent))\n",
        "\n",
        "print('Done!')\n",
        "print('{:>10,} comments'.format(len(input_ids)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gk78fzgq4io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get labels and convert from booleans to ints\n",
        "labels = train_comments.attack.to_numpy().astype(int)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjMQHP4fvJkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0a25e47b-09b7-44d9-9518-9a9f049db053"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# pad input tokens with value 0\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long',\n",
        "                          value=0, truncating='post', padding='post')\n",
        "\n",
        "print('\\nDone!')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 128 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_8HpqO1jvdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attention masks make explicit reference to which tokens are actual words vs padded words\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "\n",
        "  # mask \n",
        "  att_mask=[int(token_id > 0) for token_id in sent]\n",
        "\n",
        "  attention_masks.append(att_mask)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOWSqz4Cld9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
        "\n",
        "train_masks, val_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n",
        "\n",
        "# convert inputs and labels into torch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Wj6mUyv34J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl-hTfNzzt9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f36ff76b-a487-490b-d8af-9c9c67fc81ce"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4759pQZ0din",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                  )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyuwz3F_0tGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEkR8sRg1loZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## HELPER FUNCTIONS - accuracy & elapsed time formats\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud0J2TZ21Mtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0beb6f2f-9676-48e7-9330-ecc8a80f982c"
      },
      "source": [
        "import random\n",
        "\n",
        "# training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# store the average loss after each epoch so we can plot them\n",
        "loss_values = []\n",
        "\n",
        "# for each epoch in training...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # perform one full pass over the training set\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # measure how long the training epoch takes\n",
        "    t0 = time.time()\n",
        "\n",
        "    # reset the total loss for this epoch\n",
        "    total_loss = 0\n",
        "\n",
        "    # don't be mislead--the call to 'train' just changes the *mode*\n",
        "    # 'dropout' and 'batchnorm' layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # for each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # as we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # 'to' method.\n",
        "        #\n",
        "        # 'batch' contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # always clear any previously calculated gradients before performing a\n",
        "        # backward pass because PyTorch doesn't do this automatically\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # perform a forward pass (evaluate the model on this training batch)\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # the call to 'model' always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 'loss' is a Tensor containing a single value; \n",
        "        # the '.item()' function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the norm of the gradients to 1.0 to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    # calculate the average loss over the training data\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # store the loss value for plotting the learning curve\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # after the completion of each training epoch, measure our performance on\n",
        "    # validation set\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        \n",
        "        # add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # anpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # forward pass, calculate logit predictions\n",
        "            # this will return the logits rather than the loss because we have\n",
        "            # not provided labels\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # calculate the accuracy for this batch of test sentences\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,956.    Elapsed: 0:00:49.\n",
            "  Batch    80  of  1,956.    Elapsed: 0:01:38.\n",
            "  Batch   120  of  1,956.    Elapsed: 0:02:27.\n",
            "  Batch   160  of  1,956.    Elapsed: 0:03:17.\n",
            "  Batch   200  of  1,956.    Elapsed: 0:04:06.\n",
            "  Batch   240  of  1,956.    Elapsed: 0:04:55.\n",
            "  Batch   280  of  1,956.    Elapsed: 0:05:45.\n",
            "  Batch   320  of  1,956.    Elapsed: 0:06:35.\n",
            "  Batch   360  of  1,956.    Elapsed: 0:07:24.\n",
            "  Batch   400  of  1,956.    Elapsed: 0:08:14.\n",
            "  Batch   440  of  1,956.    Elapsed: 0:09:03.\n",
            "  Batch   480  of  1,956.    Elapsed: 0:09:53.\n",
            "  Batch   520  of  1,956.    Elapsed: 0:10:42.\n",
            "  Batch   560  of  1,956.    Elapsed: 0:11:31.\n",
            "  Batch   600  of  1,956.    Elapsed: 0:12:20.\n",
            "  Batch   640  of  1,956.    Elapsed: 0:13:09.\n",
            "  Batch   680  of  1,956.    Elapsed: 0:13:58.\n",
            "  Batch   720  of  1,956.    Elapsed: 0:14:47.\n",
            "  Batch   760  of  1,956.    Elapsed: 0:15:36.\n",
            "  Batch   800  of  1,956.    Elapsed: 0:16:25.\n",
            "  Batch   840  of  1,956.    Elapsed: 0:17:15.\n",
            "  Batch   880  of  1,956.    Elapsed: 0:18:04.\n",
            "  Batch   920  of  1,956.    Elapsed: 0:18:54.\n",
            "  Batch   960  of  1,956.    Elapsed: 0:19:43.\n",
            "  Batch 1,000  of  1,956.    Elapsed: 0:20:33.\n",
            "  Batch 1,040  of  1,956.    Elapsed: 0:21:22.\n",
            "  Batch 1,080  of  1,956.    Elapsed: 0:22:11.\n",
            "  Batch 1,120  of  1,956.    Elapsed: 0:23:01.\n",
            "  Batch 1,160  of  1,956.    Elapsed: 0:23:50.\n",
            "  Batch 1,200  of  1,956.    Elapsed: 0:24:40.\n",
            "  Batch 1,240  of  1,956.    Elapsed: 0:25:30.\n",
            "  Batch 1,280  of  1,956.    Elapsed: 0:26:19.\n",
            "  Batch 1,320  of  1,956.    Elapsed: 0:27:09.\n",
            "  Batch 1,360  of  1,956.    Elapsed: 0:27:58.\n",
            "  Batch 1,400  of  1,956.    Elapsed: 0:28:48.\n",
            "  Batch 1,440  of  1,956.    Elapsed: 0:29:38.\n",
            "  Batch 1,480  of  1,956.    Elapsed: 0:30:27.\n",
            "  Batch 1,520  of  1,956.    Elapsed: 0:31:17.\n",
            "  Batch 1,560  of  1,956.    Elapsed: 0:32:06.\n",
            "  Batch 1,600  of  1,956.    Elapsed: 0:32:56.\n",
            "  Batch 1,640  of  1,956.    Elapsed: 0:33:45.\n",
            "  Batch 1,680  of  1,956.    Elapsed: 0:34:35.\n",
            "  Batch 1,720  of  1,956.    Elapsed: 0:35:24.\n",
            "  Batch 1,760  of  1,956.    Elapsed: 0:36:14.\n",
            "  Batch 1,800  of  1,956.    Elapsed: 0:37:03.\n",
            "  Batch 1,840  of  1,956.    Elapsed: 0:37:53.\n",
            "  Batch 1,880  of  1,956.    Elapsed: 0:38:43.\n",
            "  Batch 1,920  of  1,956.    Elapsed: 0:39:32.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:40:16\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:01:36\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,956.    Elapsed: 0:00:49.\n",
            "  Batch    80  of  1,956.    Elapsed: 0:01:39.\n",
            "  Batch   120  of  1,956.    Elapsed: 0:02:28.\n",
            "  Batch   160  of  1,956.    Elapsed: 0:03:18.\n",
            "  Batch   200  of  1,956.    Elapsed: 0:04:07.\n",
            "  Batch   240  of  1,956.    Elapsed: 0:04:57.\n",
            "  Batch   280  of  1,956.    Elapsed: 0:05:46.\n",
            "  Batch   320  of  1,956.    Elapsed: 0:06:36.\n",
            "  Batch   360  of  1,956.    Elapsed: 0:07:25.\n",
            "  Batch   400  of  1,956.    Elapsed: 0:08:15.\n",
            "  Batch   440  of  1,956.    Elapsed: 0:09:04.\n",
            "  Batch   480  of  1,956.    Elapsed: 0:09:54.\n",
            "  Batch   520  of  1,956.    Elapsed: 0:10:43.\n",
            "  Batch   560  of  1,956.    Elapsed: 0:11:32.\n",
            "  Batch   600  of  1,956.    Elapsed: 0:12:22.\n",
            "  Batch   640  of  1,956.    Elapsed: 0:13:11.\n",
            "  Batch   680  of  1,956.    Elapsed: 0:14:01.\n",
            "  Batch   720  of  1,956.    Elapsed: 0:14:50.\n",
            "  Batch   760  of  1,956.    Elapsed: 0:15:40.\n",
            "  Batch   800  of  1,956.    Elapsed: 0:16:29.\n",
            "  Batch   840  of  1,956.    Elapsed: 0:17:18.\n",
            "  Batch   880  of  1,956.    Elapsed: 0:18:07.\n",
            "  Batch   920  of  1,956.    Elapsed: 0:18:56.\n",
            "  Batch   960  of  1,956.    Elapsed: 0:19:46.\n",
            "  Batch 1,000  of  1,956.    Elapsed: 0:20:35.\n",
            "  Batch 1,040  of  1,956.    Elapsed: 0:21:24.\n",
            "  Batch 1,080  of  1,956.    Elapsed: 0:22:13.\n",
            "  Batch 1,120  of  1,956.    Elapsed: 0:23:03.\n",
            "  Batch 1,160  of  1,956.    Elapsed: 0:23:52.\n",
            "  Batch 1,200  of  1,956.    Elapsed: 0:24:41.\n",
            "  Batch 1,240  of  1,956.    Elapsed: 0:25:31.\n",
            "  Batch 1,280  of  1,956.    Elapsed: 0:26:20.\n",
            "  Batch 1,320  of  1,956.    Elapsed: 0:27:09.\n",
            "  Batch 1,360  of  1,956.    Elapsed: 0:27:59.\n",
            "  Batch 1,400  of  1,956.    Elapsed: 0:28:48.\n",
            "  Batch 1,440  of  1,956.    Elapsed: 0:29:37.\n",
            "  Batch 1,480  of  1,956.    Elapsed: 0:30:27.\n",
            "  Batch 1,520  of  1,956.    Elapsed: 0:31:16.\n",
            "  Batch 1,560  of  1,956.    Elapsed: 0:32:05.\n",
            "  Batch 1,600  of  1,956.    Elapsed: 0:32:55.\n",
            "  Batch 1,640  of  1,956.    Elapsed: 0:33:44.\n",
            "  Batch 1,680  of  1,956.    Elapsed: 0:34:33.\n",
            "  Batch 1,720  of  1,956.    Elapsed: 0:35:23.\n",
            "  Batch 1,760  of  1,956.    Elapsed: 0:36:12.\n",
            "  Batch 1,800  of  1,956.    Elapsed: 0:37:01.\n",
            "  Batch 1,840  of  1,956.    Elapsed: 0:37:51.\n",
            "  Batch 1,880  of  1,956.    Elapsed: 0:38:40.\n",
            "  Batch 1,920  of  1,956.    Elapsed: 0:39:29.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:40:13\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.96\n",
            "  Validation took: 0:01:35\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,956.    Elapsed: 0:00:49.\n",
            "  Batch    80  of  1,956.    Elapsed: 0:01:39.\n",
            "  Batch   120  of  1,956.    Elapsed: 0:02:28.\n",
            "  Batch   160  of  1,956.    Elapsed: 0:03:17.\n",
            "  Batch   200  of  1,956.    Elapsed: 0:04:07.\n",
            "  Batch   240  of  1,956.    Elapsed: 0:04:56.\n",
            "  Batch   280  of  1,956.    Elapsed: 0:05:45.\n",
            "  Batch   320  of  1,956.    Elapsed: 0:06:35.\n",
            "  Batch   360  of  1,956.    Elapsed: 0:07:24.\n",
            "  Batch   400  of  1,956.    Elapsed: 0:08:13.\n",
            "  Batch   440  of  1,956.    Elapsed: 0:09:03.\n",
            "  Batch   480  of  1,956.    Elapsed: 0:09:52.\n",
            "  Batch   520  of  1,956.    Elapsed: 0:10:42.\n",
            "  Batch   560  of  1,956.    Elapsed: 0:11:31.\n",
            "  Batch   600  of  1,956.    Elapsed: 0:12:21.\n",
            "  Batch   640  of  1,956.    Elapsed: 0:13:10.\n",
            "  Batch   680  of  1,956.    Elapsed: 0:14:00.\n",
            "  Batch   720  of  1,956.    Elapsed: 0:14:49.\n",
            "  Batch   760  of  1,956.    Elapsed: 0:15:39.\n",
            "  Batch   800  of  1,956.    Elapsed: 0:16:28.\n",
            "  Batch   840  of  1,956.    Elapsed: 0:17:18.\n",
            "  Batch   880  of  1,956.    Elapsed: 0:18:07.\n",
            "  Batch   920  of  1,956.    Elapsed: 0:18:57.\n",
            "  Batch   960  of  1,956.    Elapsed: 0:19:46.\n",
            "  Batch 1,000  of  1,956.    Elapsed: 0:20:35.\n",
            "  Batch 1,040  of  1,956.    Elapsed: 0:21:25.\n",
            "  Batch 1,080  of  1,956.    Elapsed: 0:22:14.\n",
            "  Batch 1,120  of  1,956.    Elapsed: 0:23:04.\n",
            "  Batch 1,160  of  1,956.    Elapsed: 0:23:53.\n",
            "  Batch 1,200  of  1,956.    Elapsed: 0:24:43.\n",
            "  Batch 1,240  of  1,956.    Elapsed: 0:25:32.\n",
            "  Batch 1,280  of  1,956.    Elapsed: 0:26:22.\n",
            "  Batch 1,320  of  1,956.    Elapsed: 0:27:11.\n",
            "  Batch 1,360  of  1,956.    Elapsed: 0:28:01.\n",
            "  Batch 1,400  of  1,956.    Elapsed: 0:28:50.\n",
            "  Batch 1,440  of  1,956.    Elapsed: 0:29:39.\n",
            "  Batch 1,480  of  1,956.    Elapsed: 0:30:28.\n",
            "  Batch 1,520  of  1,956.    Elapsed: 0:31:17.\n",
            "  Batch 1,560  of  1,956.    Elapsed: 0:32:06.\n",
            "  Batch 1,600  of  1,956.    Elapsed: 0:32:55.\n",
            "  Batch 1,640  of  1,956.    Elapsed: 0:33:44.\n",
            "  Batch 1,680  of  1,956.    Elapsed: 0:34:33.\n",
            "  Batch 1,720  of  1,956.    Elapsed: 0:35:22.\n",
            "  Batch 1,760  of  1,956.    Elapsed: 0:36:10.\n",
            "  Batch 1,800  of  1,956.    Elapsed: 0:36:59.\n",
            "  Batch 1,840  of  1,956.    Elapsed: 0:37:48.\n",
            "  Batch 1,880  of  1,956.    Elapsed: 0:38:37.\n",
            "  Batch 1,920  of  1,956.    Elapsed: 0:39:25.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:40:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:01:34\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  1,956.    Elapsed: 0:00:49.\n",
            "  Batch    80  of  1,956.    Elapsed: 0:01:38.\n",
            "  Batch   120  of  1,956.    Elapsed: 0:02:28.\n",
            "  Batch   160  of  1,956.    Elapsed: 0:03:17.\n",
            "  Batch   200  of  1,956.    Elapsed: 0:04:06.\n",
            "  Batch   240  of  1,956.    Elapsed: 0:04:55.\n",
            "  Batch   280  of  1,956.    Elapsed: 0:05:44.\n",
            "  Batch   320  of  1,956.    Elapsed: 0:06:33.\n",
            "  Batch   360  of  1,956.    Elapsed: 0:07:22.\n",
            "  Batch   400  of  1,956.    Elapsed: 0:08:12.\n",
            "  Batch   440  of  1,956.    Elapsed: 0:09:01.\n",
            "  Batch   480  of  1,956.    Elapsed: 0:09:50.\n",
            "  Batch   520  of  1,956.    Elapsed: 0:10:39.\n",
            "  Batch   560  of  1,956.    Elapsed: 0:11:28.\n",
            "  Batch   600  of  1,956.    Elapsed: 0:12:18.\n",
            "  Batch   640  of  1,956.    Elapsed: 0:13:07.\n",
            "  Batch   680  of  1,956.    Elapsed: 0:13:56.\n",
            "  Batch   720  of  1,956.    Elapsed: 0:14:45.\n",
            "  Batch   760  of  1,956.    Elapsed: 0:15:34.\n",
            "  Batch   800  of  1,956.    Elapsed: 0:16:24.\n",
            "  Batch   840  of  1,956.    Elapsed: 0:17:13.\n",
            "  Batch   880  of  1,956.    Elapsed: 0:18:02.\n",
            "  Batch   920  of  1,956.    Elapsed: 0:18:51.\n",
            "  Batch   960  of  1,956.    Elapsed: 0:19:41.\n",
            "  Batch 1,000  of  1,956.    Elapsed: 0:20:30.\n",
            "  Batch 1,040  of  1,956.    Elapsed: 0:21:19.\n",
            "  Batch 1,080  of  1,956.    Elapsed: 0:22:08.\n",
            "  Batch 1,120  of  1,956.    Elapsed: 0:22:58.\n",
            "  Batch 1,160  of  1,956.    Elapsed: 0:23:47.\n",
            "  Batch 1,200  of  1,956.    Elapsed: 0:24:36.\n",
            "  Batch 1,240  of  1,956.    Elapsed: 0:25:26.\n",
            "  Batch 1,280  of  1,956.    Elapsed: 0:26:15.\n",
            "  Batch 1,320  of  1,956.    Elapsed: 0:27:04.\n",
            "  Batch 1,360  of  1,956.    Elapsed: 0:27:54.\n",
            "  Batch 1,400  of  1,956.    Elapsed: 0:28:43.\n",
            "  Batch 1,440  of  1,956.    Elapsed: 0:29:33.\n",
            "  Batch 1,480  of  1,956.    Elapsed: 0:30:22.\n",
            "  Batch 1,520  of  1,956.    Elapsed: 0:31:11.\n",
            "  Batch 1,560  of  1,956.    Elapsed: 0:32:01.\n",
            "  Batch 1,600  of  1,956.    Elapsed: 0:32:50.\n",
            "  Batch 1,640  of  1,956.    Elapsed: 0:33:40.\n",
            "  Batch 1,680  of  1,956.    Elapsed: 0:34:29.\n",
            "  Batch 1,720  of  1,956.    Elapsed: 0:35:18.\n",
            "  Batch 1,760  of  1,956.    Elapsed: 0:36:08.\n",
            "  Batch 1,800  of  1,956.    Elapsed: 0:36:57.\n",
            "  Batch 1,840  of  1,956.    Elapsed: 0:37:47.\n",
            "  Batch 1,880  of  1,956.    Elapsed: 0:38:36.\n",
            "  Batch 1,920  of  1,956.    Elapsed: 0:39:25.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:40:09\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:01:36\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PkRmgHB9QWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "27d37a21-d068-4a9d-8195-54c0380837fa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# learning curve\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MmGWQ31QWFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize all of the sentences and map the tokens to thier word IDs\n",
        "test_input_ids = []\n",
        "\n",
        "# for every sentence...\n",
        "for sent in test_comments.comment:\n",
        "    # 'encode' will:\n",
        "    #   (1) tokenize the comments\n",
        "    #   (2) prepend the [CLS] token to the start\n",
        "    #   (3) append the [SEP] token to the end\n",
        "    #   (4) map tokens to their IDs\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # sentence encoding\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    test_input_ids.append(encoded_sent)\n",
        "\n",
        "test_labels = test_comments.attack.to_numpy().astype(int)\n",
        "\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# create attention masks\n",
        "test_attention_masks = []\n",
        "\n",
        "# create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  test_attention_masks.append(seq_mask) \n",
        "\n",
        "# convert to tensors\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# set batch size  \n",
        "batch_size = 32  \n",
        "\n",
        "# create the DataLoader\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-TUgDY5TRPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "5cd64c11-30f8-4e8d-f625-628f7608aee0"
      },
      "source": [
        "## PREDICTION on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_inputs)))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "for (step, batch) in enumerate(test_dataloader):\n",
        "\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  if step % 100 == 0 and not step == 0:\n",
        "\n",
        "    elapsed = format_time(time.time() - t0)\n",
        "\n",
        "    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(b_input_ids, token_type_ids=None,\n",
        "                    attention_mask=b_input_mask)\n",
        "    \n",
        "    logits = outputs[0]\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    Done!')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 23,178 test sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0516d176c0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKPGERnBTbqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaYFtTt-YLmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uSJGJXrYMMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_labels[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWdUvQfvpa82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "p1 = prediction[:,1]\n",
        "\n",
        "auc = roc_auc_score(true_labels, p1)\n",
        "print('Test ROC AUC: %.3f' %auc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}