{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "biobert_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "STeCPrW4KFpJ",
        "DXPpUMx0fZOh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "icQKBKPTadOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "e00f34d6-3dc9-43b7-9f55-77e1c82c0c24"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.47)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.47->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.47->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.47->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Gn2SVI7utX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "202b4e0e-f7f0-418f-8fa0-39fbea4e09c7"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOeVnOOSfJec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a42d5638-9aac-4a94-a6b4-6e714a7ba4b7"
      },
      "source": [
        "# Get GPU device name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soCBkruYfPUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "116c2218-286d-4984-8705-28c8286c7d06"
      },
      "source": [
        "# tell Pytorch to use the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "print('We will use the GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STeCPrW4KFpJ",
        "colab_type": "text"
      },
      "source": [
        "# Import BioBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9pgQv8Iffq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d8e6a1d8-99f1-472e-9f77-cb13eff2ed2b"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-27 11:41:10--  https://docs.google.com/uc?export=download&confirm=0FSS&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.79.100, 173.194.79.102, 173.194.79.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.79.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-00-docs.googleusercontent.com/docs/securesc/e04v152jrn9tkhhcjaoquaa6tcvcluhh/bpcr9kglao0204c92ktd7cspa4gogjg3/1598528400000/13799006341648886493/12514420308214776482Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n",
            "--2020-08-27 11:41:10--  https://doc-08-00-docs.googleusercontent.com/docs/securesc/e04v152jrn9tkhhcjaoquaa6tcvcluhh/bpcr9kglao0204c92ktd7cspa4gogjg3/1598528400000/13799006341648886493/12514420308214776482Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n",
            "Resolving doc-08-00-docs.googleusercontent.com (doc-08-00-docs.googleusercontent.com)... 173.194.69.132, 2a00:1450:4013:c04::84\n",
            "Connecting to doc-08-00-docs.googleusercontent.com (doc-08-00-docs.googleusercontent.com)|173.194.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=5tv7n62tetjd2&continue=https://doc-08-00-docs.googleusercontent.com/docs/securesc/e04v152jrn9tkhhcjaoquaa6tcvcluhh/bpcr9kglao0204c92ktd7cspa4gogjg3/1598528400000/13799006341648886493/12514420308214776482Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=rpdbl8n1cfkpu09j9mh7ns2ubfprfajm [following]\n",
            "--2020-08-27 11:41:11--  https://docs.google.com/nonceSigner?nonce=5tv7n62tetjd2&continue=https://doc-08-00-docs.googleusercontent.com/docs/securesc/e04v152jrn9tkhhcjaoquaa6tcvcluhh/bpcr9kglao0204c92ktd7cspa4gogjg3/1598528400000/13799006341648886493/12514420308214776482Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=rpdbl8n1cfkpu09j9mh7ns2ubfprfajm\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.79.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-08-00-docs.googleusercontent.com/docs/securesc/e04v152jrn9tkhhcjaoquaa6tcvcluhh/bpcr9kglao0204c92ktd7cspa4gogjg3/1598528400000/13799006341648886493/12514420308214776482Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=5tv7n62tetjd2&user=12514420308214776482Z&hash=79lfkgfksa246aogdq67ba6vg3ei0448 [following]\n",
            "--2020-08-27 11:41:11--  https://doc-08-00-docs.googleusercontent.com/docs/securesc/e04v152jrn9tkhhcjaoquaa6tcvcluhh/bpcr9kglao0204c92ktd7cspa4gogjg3/1598528400000/13799006341648886493/12514420308214776482Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=5tv7n62tetjd2&user=12514420308214776482Z&hash=79lfkgfksa246aogdq67ba6vg3ei0448\n",
            "Connecting to doc-08-00-docs.googleusercontent.com (doc-08-00-docs.googleusercontent.com)|173.194.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘biobert_weights’\n",
            "\n",
            "biobert_weights         [            <=>     ] 382.81M   144MB/s    in 2.7s    \n",
            "\n",
            "2020-08-27 11:41:14 (144 MB/s) - ‘biobert_weights’ saved [401403346]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDpKJWMBEa1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f937c703-4f08-4873-ea9e-a6aeae6069bc"
      },
      "source": [
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5dhA43zEfrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9be7427f-d766-4d7d-b3ac-1ad36e3274bf"
      },
      "source": [
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-27 11:41:20.665535: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_bert:Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "INFO:transformers.modeling_bert:Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "INFO:transformers.modeling_bert:Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vqdJUV1EiEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "269b2df5-cb62-4bc1-b623-cfa6d258032c"
      },
      "source": [
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9-3E8ZPLA8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "31b3161d-e103-4471-905c-6f9706dd89a0"
      },
      "source": [
        "!ls "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "biobert_v1.1_pubmed  biobert_weights  bionlp_tags.csv  gdrive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXPpUMx0fZOh",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cSxrv3pKBx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bdab4ed3-745e-49b4-cb84-57d07b183afe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPfP8ssmdOeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 75\n",
        "BATCH_SIZE = 32\n",
        "tokenizer = BertTokenizer(vocab_file='biobert_v1.1_pubmed/vocab.txt', do_lower_case=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6aKboidi1NQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76869cb3-9306-41ad-de16-b3878bc7e5d8"
      },
      "source": [
        "data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/bio_ner/bionlp_tags.csv')\n",
        "tag_values = data['tags'].values\n",
        "vocab_len = len(tag_values)\n",
        "print('Entity Types:',vocab_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entity Types: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIVLtuLMmIC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0c39348e-911b-4d76-ce0b-d7b98aed26b6"
      },
      "source": [
        "df_tags = pd.DataFrame({'tags':tag_values})\n",
        "df_tags.to_csv('bionlp_tags.csv',index=False)\n",
        "df = pd.read_csv('bionlp_tags.csv')\n",
        "print('Tag Preview:\\n', df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tag Preview:\n",
            "                       tags\n",
            "0     I-Cellular_component\n",
            "1   E-Gene_or_gene_product\n",
            "2   I-Organism_subdivision\n",
            "3     I-Organism_substance\n",
            "4   B-Gene_or_gene_product\n",
            "..                     ...\n",
            "69                 I-Organ\n",
            "70                S-Cancer\n",
            "71            B-Amino_acid\n",
            "72     S-Anatomical_system\n",
            "73                     PAD\n",
            "\n",
            "[74 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcwhPzqRtpEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceFetch(object):\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.sentences = []\n",
        "    self.tags = []\n",
        "    self.sent = []\n",
        "    self.tag = []\n",
        "    \n",
        "    # make tsv file readable\n",
        "    with open(self.data) as tsv_f:\n",
        "      reader = csv.reader(tsv_f, delimiter='\\t')\n",
        "      for row in reader:\n",
        "        if len(row) == 0:\n",
        "          if len(self.sent) != len(self.tag):\n",
        "            break\n",
        "          self.sentences.append(self.sent)\n",
        "          self.tags.append(self.tag)\n",
        "          self.sent = []\n",
        "          self.tag = []\n",
        "        else:\n",
        "          self.sent.append(row[0])\n",
        "          self.tag.append(row[1])   \n",
        "\n",
        "  def getSentences(self):\n",
        "    return self.sentences\n",
        "  \n",
        "  def getTags(self):\n",
        "    return self.tags"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24fTBYV4wNwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpora = '/content/gdrive/My Drive/Colab Notebooks/Data/bionlp_corpora'\n",
        "sentences = []\n",
        "tags = []\n",
        "for subdir, dirs, files in os.walk(corpora):\n",
        "    for file in files:\n",
        "        if file == 'train.tsv':\n",
        "            path = os.path.join(subdir, file)\n",
        "            sent = SentenceFetch(path).getSentences()\n",
        "            tag = SentenceFetch(path).getTags()\n",
        "            sentences.extend(sent)\n",
        "            tags.extend(tag)\n",
        "            \n",
        "sentences = sentences[0:20000]\n",
        "tags = tags[0:20000]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oikx9K-B2f7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "436257c1-5c9e-4a41-83ce-50b97c296bcd"
      },
      "source": [
        "print('Sentence Preview:\\n',sentences[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence Preview:\n",
            " ['The', 'Cdc6', 'protein', 'is', 'ubiquitinated', 'in', 'vivo', 'for', 'proteolysis', 'in', 'Saccharomyces', 'cerevisiae', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDczy6Tyw-dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tok_with_labels(sent, text_labels):\n",
        "  '''tokenize and keep labels intact'''\n",
        "  tok_sent = []\n",
        "  labels = []\n",
        "  for word, label in zip(sent, text_labels):\n",
        "    tok_word = tokenizer.tokenize(word)\n",
        "    n_subwords = len(tok_word)\n",
        "\n",
        "    tok_sent.extend(tok_word)\n",
        "    labels.extend([label] * n_subwords)\n",
        "  return tok_sent, labels\n",
        "\n",
        "tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(sentences, tags)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH1VCcKpyDaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]\n",
        "labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHPtXTIByG7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0cfe7a6-0e3b-463a-9cbe-df596835b966"
      },
      "source": [
        "len(tok_texts)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pZrjN1uyNVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSIuqkC1yQHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dc17e428-a176-41c6-f3a1-4e818e001e3b"
      },
      "source": [
        "for char in tok_texts:\n",
        "    print('WordPiece Tokenizer Preview:\\n', char)\n",
        "    break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WordPiece Tokenizer Preview:\n",
            " ['The', 'C', '##d', '##c', '##6', 'protein', 'is', 'u', '##bi', '##qui', '##tina', '##ted', 'in', 'v', '##ivo', 'for', 'pro', '##te', '##oly', '##sis', 'in', 'Sa', '##cc', '##har', '##omy', '##ces', 'c', '##ere', '##vis', '##iae', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIsQlfGaDwJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_values = list(set(itertools.chain.from_iterable(tags)))\n",
        "tag_values.append(\"PAD\")\n",
        "\n",
        "tag2idx = {t: i for i,t in enumerate(tag_values)}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUlloNl1BCOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WegdV9MEBGPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attention masks make explicit reference to which tokens are actual words vs padded words\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHl9n7J7BJOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVJvgMr2BL9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlRTtznfV_L",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyG2ydyOBadA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BertConfig.from_json_file('biobert_v1.1_pubmed/config.json')\n",
        "tmp_d = torch.load('biobert_v1.1_pubmed/pytorch_model.bin', map_location=device)\n",
        "state_dict = OrderedDict()\n",
        "\n",
        "for i in list(tmp_d.keys())[:199]:\n",
        "    x = i\n",
        "    if i.find('bert') > -1:\n",
        "        x = '.'.join(i.split('.')[1:])\n",
        "    state_dict[x] = tmp_d[i]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSFH01a3bSIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BioBertNER(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_len, config, state_dict):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel(config)\n",
        "    self.bert.load_state_dict(state_dict)\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.output = nn.Linear(self.bert.config.hidden_size, vocab_len)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    encoded_layer, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    encl = encoded_layer[-1]\n",
        "    out = self.dropout(encl)\n",
        "    out = self.output(out)\n",
        "    return out, out.argmax(-1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFqxzw0hH5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa74b37e-bedb-4713-8981-bb892ca8a34f"
      },
      "source": [
        "model = BioBertNER(vocab_len,config,state_dict)\n",
        "model.to(device)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BioBertNER(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (output): Linear(in_features=768, out_features=74, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lF1ZFbWqNUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmvcc45KPunQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for step,batch in enumerate(data_loader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "        _,preds = torch.max(outputs,dim=2)\n",
        "        outputs = outputs.view(-1,outputs.shape[-1])\n",
        "        b_labels_shaped = b_labels.view(-1)\n",
        "        loss = loss_fn(outputs,b_labels_shaped)\n",
        "        correct_predictions += torch.sum(preds == b_labels)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3XQ10okPyIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(model,data_loader,loss_fn,device):\n",
        "    model = model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "            outputs,y_hat = model(b_input_ids,b_input_mask)\n",
        "        \n",
        "            _,preds = torch.max(outputs,dim=2)\n",
        "            outputs = outputs.view(-1,outputs.shape[-1])\n",
        "            b_labels_shaped = b_labels.view(-1)\n",
        "            loss = loss_fn(outputs,b_labels_shaped)\n",
        "            correct_predictions += torch.sum(preds == b_labels)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    \n",
        "    return correct_predictions.double()/len(data_loader) , np.mean(losses)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6_FdiWSQWiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9a33289e-4d57-4387-c725-6179626510d7"
      },
      "source": [
        "%%time\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "normalizer = BATCH_SIZE*MAX_LEN\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    total_loss = 0\n",
        "    print(f'======== Epoch {epoch+1}/{epochs} ========')\n",
        "    train_acc,train_loss = train_epoch(model,train_dataloader,loss_fn,optimizer,device,scheduler)\n",
        "    train_acc = train_acc/normalizer\n",
        "    print(f'Train Loss: {train_loss} Train Accuracy: {train_acc}')\n",
        "    total_loss += train_loss.item()\n",
        "    \n",
        "    avg_train_loss = total_loss / len(train_dataloader)  \n",
        "    loss_values.append(avg_train_loss)\n",
        "    \n",
        "    val_acc,val_loss = model_eval(model,valid_dataloader,loss_fn,device)\n",
        "    val_acc = val_acc/normalizer\n",
        "    print(f'Val Loss: {val_loss} Val Accuracy: {val_acc}')\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    \n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1/3 ========\n",
            "Train Loss: 0.20919665432870918 Train Accuracy: 0.9506216696269983\n",
            "Val Loss: 0.10402732750489599 Val Accuracy: 0.9574867724867725\n",
            "======== Epoch 2/3 ========\n",
            "Train Loss: 0.09399453359019164 Train Accuracy: 0.9648046181172292\n",
            "Val Loss: 0.09165017714812643 Val Accuracy: 0.9575992063492063\n",
            "======== Epoch 3/3 ========\n",
            "Train Loss: 0.08300281091730506 Train Accuracy: 0.9657171403197159\n",
            "Val Loss: 0.08882490646034952 Val Accuracy: 0.9579100529100529\n",
            "CPU times: user 8min 28s, sys: 5min 53s, total: 14min 22s\n",
            "Wall time: 14min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyow1_IfFEB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "90162f6c-3876-42ee-b395-714765590e61"
      },
      "source": [
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# learning curve\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZO9phN3eivG",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZQuYREzd_EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"In addition to their essential catalytic role in protein biosynthesis, aminoacyl-tRNA synthetases participate in numerous other functions, including regulation of gene expression and amino acid biosynthesis via transamidation pathways. Herein, we describe a class of aminoacyl-tRNA synthetase-like (HisZ) proteins based on the catalytic core of the contemporary class II histidyl-tRNA synthetase whose members lack aminoacylation activity but are instead essential components of the first enzyme in histidine biosynthesis ATP phosphoribosyltransferase (HisG). Prediction of the function of HisZ in Lactococcus lactis was assisted by comparative genomics, a technique that revealed a link between the presence or the absence of HisZ and a systematic variation in the length of the HisG polypeptide. HisZ is required for histidine prototrophy, and three other lines of evidence support the direct involvement of HisZ in the transferase function. (i) Genetic experiments demonstrate that complementation of an in-frame deletion of HisG from Escherichia coli (which does not possess HisZ) requires both HisG and HisZ from L. lactis. (ii) Coelution of HisG and HisZ during affinity chromatography provides evidence of direct physical interaction. (iii) Both HisG and HisZ are required for catalysis of the ATP phosphoribosyltransferase reaction. This observation of a common protein domain linking amino acid biosynthesis and protein synthesis implies an early connection between the biosynthesis of amino acids and proteins.\"\"\""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ixSMp8Zd1qg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "367813ae-4aa4-4bb3-b69e-b4b28f8bea81"
      },
      "source": [
        "sent_text = nltk.sent_tokenize(text)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQJDBjDOeXzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_text = []\n",
        "for sentence in sent_text:\n",
        "    tokenized_text.append(nltk.word_tokenize(sentence))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDysDKAJeaUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_preserve(sentence):\n",
        "    tokenized_sentence = []\n",
        "    \n",
        "    for word in sentence:\n",
        "        tokenized_word = tokenizer.tokenize(word)   \n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "    return tokenized_sentence"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsCunKjVgC1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_texts = [\n",
        "    tokenize_and_preserve(sent) for sent in tokenized_text\n",
        "]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDapUO6oefFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts]\n",
        "input_attentions = [[1]*len(in_id) for in_id in input_ids]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS7KQdGqerGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[1])\n",
        "new_tokens, new_labels = [], []\n",
        "for token in tokens:\n",
        "    if token.startswith(\"##\"):\n",
        "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "    else:\n",
        "        \n",
        "        new_tokens.append(token)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw23WTdMeuB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual_sentences = []\n",
        "pred_labels = []\n",
        "for x,y in zip(input_ids,input_attentions):\n",
        "    x = torch.tensor(x).cuda()\n",
        "    y = torch.tensor(y).cuda()\n",
        "    x = x.view(-1,x.size()[-1])\n",
        "    y = y.view(-1,y.size()[-1])\n",
        "    with torch.no_grad():\n",
        "        _,y_hat = model(x,y)\n",
        "    label_indices = y_hat.to('cpu').numpy()\n",
        "    \n",
        "    tokens = tokenizer.convert_ids_to_tokens(x.to('cpu').numpy()[0])\n",
        "    new_tokens, new_labels = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_labels.append(tag_values[label_idx])\n",
        "            new_tokens.append(token)\n",
        "    actual_sentences.append(new_tokens)\n",
        "    pred_labels.append(new_labels)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X12WbCxJYxb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5e2aae2-34b8-47b4-9a31-b8d847b17a69"
      },
      "source": [
        "for token, label in zip(actual_sentences, pred_labels):\n",
        "    for t,l in zip(token,label):\n",
        "        print(\"{}\\t{}\".format(t, l))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In\tO\n",
            "addition\tO\n",
            "to\tO\n",
            "their\tO\n",
            "essential\tO\n",
            "catalytic\tO\n",
            "role\tO\n",
            "in\tO\n",
            "protein\tO\n",
            "biosynthesis\tO\n",
            ",\tO\n",
            "aminoacyl\tO\n",
            "-\tO\n",
            "tRNA\tO\n",
            "synthetases\tO\n",
            "participate\tO\n",
            "in\tO\n",
            "numerous\tO\n",
            "other\tO\n",
            "functions\tO\n",
            ",\tO\n",
            "including\tO\n",
            "regulation\tO\n",
            "of\tO\n",
            "gene\tO\n",
            "expression\tO\n",
            "and\tO\n",
            "amino\tO\n",
            "acid\tO\n",
            "biosynthesis\tO\n",
            "via\tO\n",
            "transamidation\tO\n",
            "pathways\tO\n",
            ".\tO\n",
            "Herein\tO\n",
            ",\tO\n",
            "we\tO\n",
            "describe\tO\n",
            "a\tO\n",
            "class\tO\n",
            "of\tO\n",
            "aminoacyl\tO\n",
            "-\tO\n",
            "tRNA\tO\n",
            "synthetase\tO\n",
            "-\tO\n",
            "like\tO\n",
            "(\tO\n",
            "HisZ\tO\n",
            ")\tO\n",
            "proteins\tO\n",
            "based\tO\n",
            "on\tO\n",
            "the\tO\n",
            "catalytic\tO\n",
            "core\tO\n",
            "of\tO\n",
            "the\tO\n",
            "contemporary\tO\n",
            "class\tO\n",
            "II\tO\n",
            "histidyl\tO\n",
            "-\tO\n",
            "tRNA\tO\n",
            "synthetase\tO\n",
            "whose\tO\n",
            "members\tO\n",
            "lack\tO\n",
            "aminoacylation\tO\n",
            "activity\tO\n",
            "but\tO\n",
            "are\tO\n",
            "instead\tO\n",
            "essential\tO\n",
            "components\tO\n",
            "of\tO\n",
            "the\tO\n",
            "first\tO\n",
            "enzyme\tO\n",
            "in\tO\n",
            "histidine\tO\n",
            "biosynthesis\tO\n",
            "ATP\tO\n",
            "phosphoribosyltransferase\tO\n",
            "(\tO\n",
            "HisG\tO\n",
            ")\tO\n",
            ".\tO\n",
            "Prediction\tO\n",
            "of\tO\n",
            "the\tO\n",
            "function\tO\n",
            "of\tO\n",
            "HisZ\tS-Protein\n",
            "in\tO\n",
            "Lactococcus\tO\n",
            "lactis\tO\n",
            "was\tO\n",
            "assisted\tO\n",
            "by\tO\n",
            "comparative\tO\n",
            "genomics\tO\n",
            ",\tO\n",
            "a\tO\n",
            "technique\tO\n",
            "that\tO\n",
            "revealed\tO\n",
            "a\tO\n",
            "link\tO\n",
            "between\tO\n",
            "the\tO\n",
            "presence\tO\n",
            "or\tO\n",
            "the\tO\n",
            "absence\tO\n",
            "of\tO\n",
            "HisZ\tS-Protein\n",
            "and\tO\n",
            "a\tO\n",
            "systematic\tO\n",
            "variation\tO\n",
            "in\tO\n",
            "the\tO\n",
            "length\tO\n",
            "of\tO\n",
            "the\tO\n",
            "HisG\tS-Protein\n",
            "polypeptide\tO\n",
            ".\tO\n",
            "HisZ\tS-Protein\n",
            "is\tO\n",
            "required\tO\n",
            "for\tO\n",
            "histidine\tO\n",
            "prototrophy\tO\n",
            ",\tO\n",
            "and\tO\n",
            "three\tO\n",
            "other\tO\n",
            "lines\tO\n",
            "of\tO\n",
            "evidence\tO\n",
            "support\tO\n",
            "the\tO\n",
            "direct\tO\n",
            "involvement\tO\n",
            "of\tO\n",
            "HisZ\tS-Protein\n",
            "in\tO\n",
            "the\tO\n",
            "transferase\tO\n",
            "function\tO\n",
            ".\tO\n",
            "(\tO\n",
            "i\tO\n",
            ")\tO\n",
            "Genetic\tO\n",
            "experiments\tO\n",
            "demonstrate\tO\n",
            "that\tO\n",
            "complementation\tO\n",
            "of\tO\n",
            "an\tO\n",
            "in\tO\n",
            "-\tO\n",
            "frame\tO\n",
            "deletion\tO\n",
            "of\tO\n",
            "HisG\tS-Protein\n",
            "from\tO\n",
            "Escherichia\tO\n",
            "coli\tO\n",
            "(\tO\n",
            "which\tO\n",
            "does\tO\n",
            "not\tO\n",
            "possess\tO\n",
            "HisZ\tS-Protein\n",
            ")\tO\n",
            "requires\tO\n",
            "both\tO\n",
            "HisG\tS-Protein\n",
            "and\tO\n",
            "HisZ\tS-Protein\n",
            "from\tO\n",
            "L\tO\n",
            ".\tO\n",
            "lactis\tO\n",
            ".\tO\n",
            "(\tO\n",
            "ii\tO\n",
            ")\tO\n",
            "Coelution\tO\n",
            "of\tO\n",
            "HisG\tO\n",
            "and\tO\n",
            "HisZ\tO\n",
            "during\tO\n",
            "affinity\tO\n",
            "chromatography\tO\n",
            "provides\tO\n",
            "evidence\tO\n",
            "of\tO\n",
            "direct\tO\n",
            "physical\tO\n",
            "interaction\tO\n",
            ".\tO\n",
            "(\tO\n",
            "iii\tO\n",
            ")\tO\n",
            "Both\tO\n",
            "HisG\tO\n",
            "and\tO\n",
            "HisZ\tO\n",
            "are\tO\n",
            "required\tO\n",
            "for\tO\n",
            "catalysis\tO\n",
            "of\tO\n",
            "the\tO\n",
            "ATP\tO\n",
            "phosphoribosyltransferase\tO\n",
            "reaction\tO\n",
            ".\tO\n",
            "This\tO\n",
            "observation\tO\n",
            "of\tO\n",
            "a\tO\n",
            "common\tO\n",
            "protein\tO\n",
            "domain\tO\n",
            "linking\tO\n",
            "amino\tO\n",
            "acid\tO\n",
            "biosynthesis\tO\n",
            "and\tO\n",
            "protein\tO\n",
            "synthesis\tO\n",
            "implies\tO\n",
            "an\tO\n",
            "early\tO\n",
            "connection\tO\n",
            "between\tO\n",
            "the\tO\n",
            "biosynthesis\tO\n",
            "of\tO\n",
            "amino\tO\n",
            "acids\tO\n",
            "and\tO\n",
            "proteins\tO\n",
            ".\tO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1f42dmKPhOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save = 'BIONER_classifier.pt'\n",
        "path = F\"models/{model_save}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}